# scraper_pinterest.py
# À ajouter dans votre projet pour VRAIMENT scraper Pinterest avec Selenium

import time
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import logging

logger = logging.getLogger(__name__)

def scrape_pinterest_with_selenium(url, limit=1000, scroll_pause=2):
    """
    Scrappe Pinterest avec Selenium pour récupérer VRAIMENT toutes les images
    
    Args:
        url: URL de recherche Pinterest (ex: https://www.pinterest.fr/search/pins/?q=anime%20girl)
        limit: Nombre max d'images à récupérer (défaut 1000)
        scroll_pause: Temps d'attente entre chaque scroll (secondes)
    
    Returns:
        Liste d'URLs d'images uniques
    """
    
    # Configuration Chrome headless (sans interface)
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # Mode sans fenêtre
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
    
    driver = None
    image_urls = set()  # Set pour éviter les doublons automatiquement
    
    try:
        logger.info(f"Démarrage scraping Pinterest: {url} (limite: {limit})")
        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        
        # Attendre que la page charge
        wait = WebDriverWait(driver, 10)
        wait.until(EC.presence_of_element_all_located((By.TAG_NAME, "img")))
        
        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_attempts = 0
        max_scroll_attempts = 50  # Limite pour éviter boucle infinie
        
        while len(image_urls) < limit and scroll_attempts < max_scroll_attempts:
            # Scroll vers le bas
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(scroll_pause)
            
            # Extraire les images actuellement visibles
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # Méthode 1: Chercher les images dans <img>
            for img in soup.find_all('img'):
                src = img.get('src') or img.get('srcset')
                if src:
                    # Extraire l'URL principale si srcset
                    if ',' in src:
                        src = src.split(',')[0].strip().split(' ')[0]
                    
                    # Filtrer uniquement les vraies images Pinterest
                    if 'pinimg.com' in src and 'x/' not in src:  # Éviter les placeholders
                        # Prendre la version haute qualité (736x au lieu de 236x)
                        src = src.replace('/236x/', '/736x/')
                        image_urls.add(src)
            
            # Méthode 2: Regex sur tout le HTML pour trouver les URLs i.pinimg.com
            urls_found = re.findall(r'https://i\.pinimg\.com/[^"\'>\s]+', driver.page_source)
            for u in urls_found:
                # Prendre version haute qualité
                u = u.replace('/236x/', '/736x/')
                if '/736x/' in u or '/originals/' in u:
                    image_urls.add(u)
            
            logger.info(f"Images trouvées: {len(image_urls)}/{limit}")
            
            # Vérifier si on a atteint le bas de la page
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                scroll_attempts += 1
                if scroll_attempts >= 3:
                    logger.info("Fin de page atteinte")
                    break
            else:
                scroll_attempts = 0
            
            last_height = new_height
            
            # Pause si on a assez d'images
            if len(image_urls) >= limit:
                break
        
        # Convertir set en liste et limiter
        final_urls = list(image_urls)[:limit]
        logger.info(f"Scraping terminé: {len(final_urls)} images récupérées")
        return final_urls
        
    except Exception as e:
        logger.exception(f"Erreur scraping Pinterest: {e}")
        return list(image_urls)  # Retourner ce qu'on a pu récupérer
        
    finally:
        if driver:
            driver.quit()


def scrape_pinterest_simple(url, limit=200):
    """
    Version simplifiée sans Selenium (celle que vous avez actuellement)
    Garde pour fallback si Selenium ne marche pas
    """
    import requests
    from bs4 import BeautifulSoup
    
    HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    try:
        r = requests.get(url, headers=HEADERS, timeout=15)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, 'html.parser')
        
        urls = set()
        
        # Meta tags
        for m in soup.find_all('meta', {"property": "og:image"}):
            urls.add(m.get('content'))
        
        # Images
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src and 'pinimg.com' in src:
                src = src.replace('/236x/', '/736x/')
                urls.add(src)
        
        # Regex
        found = re.findall(r'https://i\.pinimg\.com/[^"\'>\s]+', r.text)
        for u in found:
            u = u.replace('/236x/', '/736x/')
            if '/736x/' in u or '/originals/' in u:
                urls.add(u)
        
        return list(urls)[:limit]
        
    except Exception as e:
        logger.exception(f"Erreur scraping simple: {e}")
        return []


# ===================================================================
# MISE À JOUR DE VOTRE main.py
# Remplacez la fonction scrape_pinterest par ceci:
# ===================================================================

def scrape_pinterest_smart(url, limit=200, use_selenium=True):
    """
    Scrape intelligent: essaie Selenium d'abord, sinon fallback sur requests
    
    Args:
        url: URL Pinterest
        limit: Nombre d'images max
        use_selenium: Si True, utilise Selenium (recommandé)
    
    Returns:
        Liste d'URLs d'images
    """
    
    # Si limite > 100, force Selenium car requests ne suffit pas
    if limit > 100:
        use_selenium = True
    
    if use_selenium:
        try:
            logger.info("Tentative scraping avec Selenium...")
            urls = scrape_pinterest_with_selenium(url, limit)
            if urls:
                return urls
            else:
                logger.warning("Selenium n'a rien trouvé, fallback vers requests")
        except Exception as e:
            logger.warning(f"Selenium a échoué ({e}), fallback vers requests")
    
    # Fallback: méthode simple
    logger.info("Scraping avec requests (limite ~50 images)")
    return scrape_pinterest_simple(url, min(limit, 200))


# ===================================================================
# EXEMPLE D'UTILISATION dans votre /scrape endpoint
# ===================================================================

"""
@app.route('/scrape', methods=['POST'])
def scrape_endpoint():
    data = request.get_json(force=True, silent=True) or {}
    url = data.get('url')
    category = (data.get('category') or 'uncategorized').strip()
    limit = int(data.get('limit') or 50)
    
    if not url:
        return jsonify({"error": "url manquante"}), 400
    
    logger.info(f"Scraping: {url} (cat={category} limit={limit})")
    
    # NOUVELLE VERSION avec Selenium
    found = scrape_pinterest_smart(url, limit=limit, use_selenium=True)
    
    if not found:
        return jsonify({"inserted": 0, "found": 0, "message": "Aucune image"}), 200
    
    # Insertion en DB (votre code existant)
    conn = get_db_connection()
    cur = conn.cursor()
    inserted = 0
    
    for img_url in found:
        try:
            cur.execute("SELECT id FROM images WHERE image_url = %s", (img_url,))
            if cur.fetchone():
                continue
            cur.execute(
                "INSERT INTO images (image_url, category, status) VALUES (%s,%s,%s)",
                (img_url, category, 'pending')
            )
            inserted += 1
        except:
            pass
    
    conn.commit()
    cur.close()
    return_db_connection(conn)
    
    return jsonify({"inserted": inserted, "found": len(found)}), 200
"""
